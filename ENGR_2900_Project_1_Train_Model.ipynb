{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJo-YN9POMYb"
      },
      "source": [
        "# Training a model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9t4J04lIz1SB"
      },
      "source": [
        "In this notebook, we will train a convolutional neural network for facial recognition using a technique known as transfer learning. Specifically the model will classify whether an image of a detected face is the designated celebrity or is not the designated celebrity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5HpXfVo0ffi"
      },
      "source": [
        "Transfer learning is a technique where we can use a model pre-trained for one task, and repurpose it on a new and related task. This will allow us to achieve higher performance than training a model from scratch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg61dubt69yw"
      },
      "source": [
        "Below is a nice introduction into the definition of transfer learning, taken from the following resource: https://cs231n.github.io/transfer-learning/\n",
        "\n",
        " \"In practice, very few people train an entire Convolutional Network from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9IEPiDW2Y-f"
      },
      "source": [
        "To implement the training of our model, we will use the PyTorch framework. Here is a reference for an example using PyTorch: https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc-zn-lH4aRU"
      },
      "source": [
        "The pre-trained model we will leverage on is Inception ResNet v1, which has been trained on the VGGFace 2 dataset. The VGGFace2 dataset consists of approximately 3.3M faces and 9000 classes. This model has been trained to extract features from face images. Here is the VGGFace2 dataset website: https://www.robots.ox.ac.uk/~vgg/data/vgg_face2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NhgDJjY306c"
      },
      "source": [
        "## 1. Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7yJ2b0ZtQl4Q"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m/Users/sahithjagarlamudi/Downloads/ENGR_2900_Project_1_Train_Model.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sahithjagarlamudi/Downloads/ENGR_2900_Project_1_Train_Model.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Mount drive.\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sahithjagarlamudi/Downloads/ENGR_2900_Project_1_Train_Model.ipynb#X10sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgoogle\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolab\u001b[39;00m \u001b[39mimport\u001b[39;00m drive\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sahithjagarlamudi/Downloads/ENGR_2900_Project_1_Train_Model.ipynb#X10sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m drive\u001b[39m.\u001b[39mmount(\u001b[39m'\u001b[39m\u001b[39m/content/gdrive\u001b[39m\u001b[39m'\u001b[39m, force_remount\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "# Mount drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmD_OAtR2bLE"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZv7O0IpQnlx"
      },
      "outputs": [],
      "source": [
        "# Change to your folder directory.\n",
        "%cd gdrive/MyDrive/Project-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpG9jtZiQphc"
      },
      "outputs": [],
      "source": [
        "!pip install facenet_pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxvzgkN7P-_f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "from PIL import Image\n",
        "from tempfile import TemporaryDirectory\n",
        "\n",
        "cudnn.benchmark = True\n",
        "plt.ion()   # Interactive mode.\n",
        "\n",
        "import cv2\n",
        "import copy\n",
        "from facenet_pytorch import InceptionResnetV1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQhjnTER7rZi"
      },
      "source": [
        "# 2. Load data and visualize sample images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrX34nFjNgTq"
      },
      "source": [
        "The dataset should be split into train and val folders, with each folder containing a folder for each class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4KixTvSOUOd"
      },
      "outputs": [],
      "source": [
        "# Data augmentation and normalization for training data.\n",
        "# Normalization only for validation data.\n",
        "data_transforms = {\n",
        "  'train': transforms.Compose([\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ]),\n",
        "  'val': transforms.Compose([\n",
        "      transforms.Resize(299),\n",
        "      transforms.CenterCrop(299),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "  ]),\n",
        "}\n",
        "\n",
        "data_dir = 'face_images'\n",
        "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
        "                                          data_transforms[x])\n",
        "                  for x in ['train', 'val']}\n",
        "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
        "                                             shuffle=True, num_workers=4, drop_last=True)\n",
        "              for x in ['train', 'val']}\n",
        "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
        "\n",
        "class_names = image_datasets['train'].classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "954-6nU08AmX"
      },
      "outputs": [],
      "source": [
        "print(f\"Number of images in train dataloader: {len(dataloaders['train'])}\")\n",
        "print(f\"Number of images in val dataloader: {len(dataloaders['val'])}\")\n",
        "print(f'Class names: {class_names}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjVLsVV6U32Z"
      },
      "outputs": [],
      "source": [
        "# Set device to GPUs if avaialable.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using \" + str(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdSOq2d2U7au"
      },
      "outputs": [],
      "source": [
        "# Function to display images.\n",
        "def imshow(inp, title=None):\n",
        "  \"\"\"Imshow for Tensor.\"\"\"\n",
        "  inp  = inp.numpy().transpose((1, 2, 0))\n",
        "  mean = np.array([0.485, 0.456, 0.406])\n",
        "  std  = np.array([0.229, 0.224, 0.225])\n",
        "  inp  = std * inp + mean\n",
        "  inp  = np.clip(inp, 0, 1)\n",
        "  plt.imshow(inp)\n",
        "  if title is not None:\n",
        "      plt.title(title)\n",
        "  plt.pause(5)  # Pause a bit so that visualizations are updated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eK18n4AU9oa"
      },
      "outputs": [],
      "source": [
        "# Get a batch of training data.\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch.\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "# Visualize sampel images.\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "705AhWKoA22G"
      },
      "source": [
        "## 3. Train and evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSIaieb9VBl_"
      },
      "outputs": [],
      "source": [
        "# Function to train model.\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
        "  since = time.time()\n",
        "\n",
        "  #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_acc = 0.0\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "    print('-' * 10)\n",
        "\n",
        "    # Each epoch has a training and validation phase\n",
        "    for phase in ['train', 'val']:\n",
        "      if phase == 'train':\n",
        "        model.train()  # Set model to training mode.\n",
        "      else:\n",
        "        model.eval()   # Set model to evaluate mode.\n",
        "\n",
        "      running_loss = 0.0\n",
        "      running_corrects = 0\n",
        "\n",
        "      # Iterate over data.\n",
        "      for inputs, labels in dataloaders[phase]:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients.\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass.\n",
        "        # Track history if only in train.\n",
        "        with torch.set_grad_enabled(phase == 'train'):\n",
        "          outputs = model(inputs)\n",
        "          _, preds = torch.max(outputs, 1)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backpropagation only in train.\n",
        "          if phase == 'train':\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "          # statistics\n",
        "          running_loss += loss.item() * inputs.size(0)\n",
        "          running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "        if phase == 'train':\n",
        "          scheduler.step()\n",
        "\n",
        "        epoch_loss = running_loss / dataset_sizes[phase]\n",
        "        epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "        print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "          phase, epoch_loss, epoch_acc))\n",
        "\n",
        "        # Deep copy the model.\n",
        "        if phase == 'val' and epoch_acc > best_acc:\n",
        "          best_acc = epoch_acc\n",
        "          best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "      print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RbYI8FgqVD2g"
      },
      "outputs": [],
      "source": [
        "# Function to visualize model predictions on validation set.\n",
        "def visualize_model(model, num_images=6):\n",
        "  was_training = model.training\n",
        "  model.eval()\n",
        "  images_so_far = 0\n",
        "  fig = plt.figure()\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(inputs)\n",
        "      _, preds = torch.max(outputs, 1)\n",
        "      print(f'preds: {preds}')\n",
        "\n",
        "      for j in range(inputs.size()[0]):\n",
        "        images_so_far += 1\n",
        "        ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "        ax.axis('off')\n",
        "        ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "        imshow(inputs.cpu().data[j])\n",
        "\n",
        "        if images_so_far == num_images:\n",
        "          model.train(mode=was_training)\n",
        "          return\n",
        "    model.train(mode=was_training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO_2AfmtVGd9"
      },
      "outputs": [],
      "source": [
        "# Setup model\n",
        "# Specify the number of classes.\n",
        "model_ft = InceptionResnetV1(pretrained='vggface2', device=device, classify= True, num_classes=len(class_names))\n",
        "\n",
        "print(model_ft.logits)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Setup loss function.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized.\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs.\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQTtf15HDXtb"
      },
      "outputs": [],
      "source": [
        "# Train model.\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=10)\n",
        "\n",
        "# Save the trained model to be used later for inference.\n",
        "model_path = \"trained_model.pt\"\n",
        "print(\"Saving model \"+model_path);\n",
        "torch.save(model_ft.state_dict(), model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hd3iZbziDwvJ"
      },
      "outputs": [],
      "source": [
        "# Visualize model on validation data.\n",
        "visualize_model(model_ft)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
